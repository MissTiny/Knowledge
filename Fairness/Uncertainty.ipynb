{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20db87c-197d-447d-ab2b-4bc63d215abf",
   "metadata": {},
   "source": [
    "# Uncertainty as Fairness Measure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9812746-4c19-4542-bd17-34b0807efd1d",
   "metadata": {},
   "source": [
    "[Uncertainty as a Fairness Measure by Slim Kuzucu et al. 2024](https://arxiv.org/html/2312.11299v2) introduced *uncertainty-based* fairness measures at the group and individual levels, which is complementary to point-based measures. They showed that models can be fair with point-based fairness measures but biased against demographic groups regarding prediction uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad39fab-337e-4fce-a22f-5aec414f1e27",
   "metadata": {},
   "source": [
    "## Measuring Group Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb247ba-da2e-42aa-abf3-ed2afc853ecb",
   "metadata": {},
   "source": [
    "An ML model can be considered **fair** if a chosen performance measure for a specific task is the same across different groups.\n",
    "$$Fair(f;M,D) \\rightarrow M(D,f,G=0) = M(D,f,G=1)$$\n",
    ",where $f$ is ML model, $M$ is the given performance, $G=1$ is the majority group, and $G=0$ is the miniority group, $D$ is the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee2ebe-179f-4e2f-a2e8-eab59f2d1c8a",
   "metadata": {},
   "source": [
    "**Statistical Parity or Demographic Parity**\n",
    "\n",
    "The model's prediction probabilities for positive class ($\\hat{Y}=1$) across different groups are the *same*.\n",
    "$$P(\\hat{Y}=1|G = 0) = P(\\hat{Y}=1|G=1)$$\n",
    ", where $M(D,f,G) = P(\\hat{Y}=1 | G)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367f56d-d26d-49d9-816c-304e6d1741da",
   "metadata": {},
   "source": [
    "**Equal Opportunity**\n",
    "\n",
    "The model's false negative rates i.e, prediction probabilities for the negative class ($\\hat{Y} = 0$) for the known positive class ($Y=1$) are the *same*.\n",
    "$$P(\\hat{Y}=0 | Y=1,G=0) = P(\\hat{Y}=0 | Y=1,G=1)$$\n",
    ",where $M(D,f,G) = P(\\hat{Y}=0 | Y=1,G)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e145b-89ed-4e66-8bc0-ea3a6e637ddc",
   "metadata": {},
   "source": [
    "**Equalised Odds**\n",
    "\n",
    "The model's prediction probabilities for the positive class ($\\hat{Y}=1$) for different ground truth classes ($Y=1$ and $Y=0$) are the same\n",
    "$$P(\\hat{Y}=1 | Y=y,G=0) = P(\\hat{Y}=1 | Y=y,G=1)$$\n",
    ", where $y \\in {0,1}$, and $M(D,f,G) = P(\\hat{Y}=1 | Y=y,G)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635ec78-f786-4067-9c9b-8ad4ef170671",
   "metadata": {},
   "source": [
    "## Measuring Individual Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e54aca-ed44-4d74-a133-f7ed56c33e60",
   "metadata": {},
   "source": [
    "[Dwork et al 2012](https://arxiv.org/abs/1104.3913) **Individual Fairness** based on a \"similar individuals should have similar predictions\" principle :\n",
    "\n",
    "$$d_y(f(\\pmb{x_1}),f(\\pmb{x_2})) \\leq Ld_x(\\pmb{x_1},\\pmb{x_2}), \\forall \\pmb{x_1},\\pmb{x_2} \\in \\mathscr{X}$$\n",
    ", where $d_y(\\cdot ,\\cdot )$ and $d_x(\\cdot,\\cdot)$ are distance metrics for predictions and inputs.\n",
    "\n",
    "The measurement can be achieved with point predictions:\n",
    "$$\\mathscr{F}_{\\hat{y}}^{indv}(X=\\pmb{x_i}) = 1 - |\\hat{y}_i - \\frac{1}{k} \\sum_{\\pmb{x_j} \\in kNN(\\pmb{x_i})} \\hat{y}_j |$$\n",
    ", where $kNN(\\pmb{x_i})$ denotes the k-nearest neighbors of $\\pmb{x_i}$\n",
    "\n",
    "If neighbours can accurately estimate $x_i$, then $\\frac{1}{k} \\sum_{\\pmb{x_j} \\in kNN(\\pmb{x_i})} \\hat{y}_j$ will be close to $\\hat{y}_i$, $|\\hat{y}_i - \\frac{1}{k} \\sum_{\\pmb{x_j} \\in kNN(\\pmb{x_i})} \\hat{y}_j |$ will be close to 0, and thus $\\mathscr{F}_{\\hat{y}}^{indv}(X=\\pmb{x_i})$ will be close to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47e028-a266-42c1-b890-1801ceb27e76",
   "metadata": {},
   "source": [
    "## Quantifying Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa1c6a2-b6aa-4499-8808-4985ce406565",
   "metadata": {},
   "source": [
    "**Epistemic or model Uncertainty** is measured over different models. It reflects the lack of knowledge about the current input and can be reduced by providing more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903407d-3eb5-4f77-ae02-9f67ae99b8a3",
   "metadata": {},
   "source": [
    "**Aleatoric or data uncertainty** is measured over classes. Aleatoric uncertainty reflects the irreducible noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc5ae8a-ee71-4e0e-8bdb-c99021eb5dea",
   "metadata": {},
   "source": [
    "[Kuzucu et al. 2024](https://arxiv.org/html/2312.11299v2) use Bayesian Neural Networks (BNNs) to obtain uncertainty estimates described in [Blundell et al 2015](https://arxiv.org/abs/1505.05424). A BNN defines a distribution over each weight in the model: $w_i = (\\mu_i,\\sigma_i)$, which enables sampling different weights and making multiple predictions for the same input. With BNN, **predictive uncertainty** ($\\mu_p$) for a sample $x$ with label $y$ can be quantified as:\n",
    "$$\\frac{1}{M}\\sum_{m=1}^M (P_m - \\bar{P})^T (P_m - \\bar{P}) + \\frac{1}{M} \\sum_{m=1}^M diag(P_m) - P^T_m \\cdot P_m$$\n",
    ", where $\\bar{P} = \\frac{1}{M}\\sum_{m=1}^M P_m$ and $P_m = P(Y | X=x)$ of the $m^{th}$ Monte Carlo sample with M being the number of Monte Carlo samples. \n",
    "\n",
    "$\\frac{1}{M}\\sum_{m=1}^M (P_m - \\bar{P})^T (P_m - \\bar{P})$ is the **Epistemic uncertatinty**, denoted as $\\mu_e$. It calculates the differences between each sample model and the average, which is the variance of sample models.\n",
    "\n",
    "$\\frac{1}{M} \\sum_{m=1}^M diag(P_m) - P^T_m \\cdot P_m$ as **Aleatoric uncertainty**, denoted as $\\mu_a$. It calculates the uncertainty within the models. <font color='red'>What does it mean?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22091081-23e6-41fb-ab5c-defcf7105b18",
   "metadata": {},
   "source": [
    "**Uncertainty-based Group Fairness Measures**\n",
    "A model is fair if its uncertainties are the same across different groups.\n",
    "$$Fair(f;\\mu,D) = \\mu(D,f,G=0) = \\mu(D,f,G=1)$$\n",
    ", where $\\mu$ is an uncertainty measure, e.g. predictive uncertainty ($\\mu_p$), epistemic uncertainty($\\mu_e$), aleatoric uncertainty (($\\mu_a$))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ab40e-097b-4bff-93fe-0ae265116e5a",
   "metadata": {},
   "source": [
    "**Uncertainty-based Individual Fairness**\n",
    "\n",
    "$$\\mathscr{F}_{\\mu}^{indv}(X=\\pmb{x_i}) = 1 - |\\mu_i - \\frac{1}{k} \\sum_{\\pmb{x_j} \\in kNN(\\pmb{x_i})} \\mu_j |$$\n",
    ", which aggregate over a group by averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16059021-6f15-4808-95d1-e1fd2ed29738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
