{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ab300e-e58a-4fc8-9c64-d65beb99c52d",
   "metadata": {},
   "source": [
    "# Deep Generative Model\n",
    "This is a study note for Stanford CS236 Deep Generative Model.\n",
    "Additional Resources:\n",
    "[Course Github Notes](https://deepgenerativemodels.github.io/notes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308b170-4220-4cf6-bab2-08a2ea21c843",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Module 1: Introduction to Generative Models\n",
    "- Suggest Readings:\n",
    "    - [Deep Generative Models](https://ermongroup.github.io/generative-models/)\n",
    "    - [Generative Modeling by Estimating Gradients of the Data Distribution](https://yang-song.net/blog/2021/score/)\n",
    "    - [Tutorial on Deep Generative Models](https://www.youtube.com/watch?v=JrO5fSskISY)\n",
    "    - [Learning Deep Generative Models](https://www.cs.cmu.edu/~rsalakhu/papers/annrev.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d802ea3a-8704-443f-9a9b-00e25a53c120",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What is Generative Modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b95b5-03bd-44e7-97dc-773a613b7d33",
   "metadata": {},
   "source": [
    "**Generative Models** contains two parts:\n",
    "- **Generation (graphics)**: From high level description to raw sensory outputs \n",
    "- **Inference (vision as inverse graphics)**: From Raw sensory outputs to high level descriptions.\n",
    "\n",
    "**Statistical** Generative Models are **learned from data**.\n",
    "This course depends less on prior data but computer graphics deeps on more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f35f8-4978-43c4-bb90-f247045ebf23",
   "metadata": {},
   "source": [
    "#### Statistical Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52c910-ba2f-47eb-b1fa-4a10c8f51ad0",
   "metadata": {},
   "source": [
    "\n",
    "A **statistical generative** model is a probability distribution $p(x)$:\n",
    "- **Data**: Samples\n",
    "- **Prior Knowledge**: parametric form, loss function, optimization algorithm\n",
    "\n",
    "Image $x$ $\\rightarrow$ A probability distribution $p(x)$ $\\rightarrow$ scalar probability $p(x)$.\n",
    "\n",
    "It is generative because sampling from $p(x)$ generates new images.\n",
    "\n",
    "It can be used to build a simulator for the data-generating process.\n",
    "\n",
    "Control Signals/Potential datapoints $\\rightarrow$ Data Simulator = Statistical Model = Generative Model $\\rightarrow$ New datapoints/Probability values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf514e95-ca16-47f7-b87b-248113dce09e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Audio and Image Applications of Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c1b1c-5ddf-426c-8a17-11b109e62df2",
   "metadata": {},
   "source": [
    "- Data Generation in the real world\n",
    "    - Text to Image: [Language-guided artwork creation](https://chainbreakers.kath.io/)\n",
    "    - Draw Image to Realistic Images[Meng, He, Song et al ICLR 2022](https://arxiv.org/abs/2108.01073)\n",
    "- Solving inverse problems with generative models\n",
    "    - Medical image reconstruction [Song et al ICLR 2022](https://arxiv.org/abs/2111.08005)\n",
    "- Outlier Detection with genertive models\n",
    "    - Outlier Detection [Song et al ICLR 2018](https://arxiv.org/abs/1710.10766)\n",
    "- Progress in Generative Models of Images\n",
    "    - GANs [Ian Goodfellow 2019](https://arxiv.org/abs/1406.2661)\n",
    "    - Diffusion Models [Song et al 2021](https://arxiv.org/abs/2101.09258)\n",
    "        - Text2Image Diffusion Models\n",
    "- Progress in Inverse Problems\n",
    "    -  Low Resolution $\\rightarrow$ High resolution [Menon et al, 2020](https://arxiv.org/abs/2003.03808)\n",
    "    -  Mask $\\rightarrow$ Full Image [Liu et al 2018](https://arxiv.org/abs/1804.07723)\n",
    "    -  Greyscale Images $\\rightarrow$  Color Image\n",
    "    -  Scatch $\\rightarrow$ Fine Image\n",
    "    -  Origin Images $\\rightarrow$ Edited Images\n",
    "-  Audio\n",
    "    - WaveNet[van den Oord et al 2016c](https://arxiv.org/abs/1609.03499)\n",
    "    - Diffusion Text2Speech [Betker, Better Speech Synthesis through scaling 2023](https://arxiv.org/abs/2305.07243)\n",
    "    - Conditional Generative Model: Low-Resolution Audio Signal $\\rightarrow$ High-Resolution Audio Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af7571-94e6-4d2f-9963-28702c2f5802",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Language, Video, and Robotic Applications of Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd890d-1ff2-46bb-a5c7-85c283bf8ba7",
   "metadata": {},
   "source": [
    "- Language Generation [Radford et al 2019](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf)\n",
    "    - Conditional Generative Model *P(next word | previous word)*\n",
    "    - ChatGPT\n",
    "- Machine Translation\n",
    "    - Conditional Generative Model *P(English Text | Chinese Text)*\n",
    "- Code Generation\n",
    "- Video Generation\n",
    "- Imitation Learning\n",
    "    - Conditional Generative Model *P(actions | past observations)* [Li et al 2017](https://arxiv.org/abs/1701.01036) | [Janner et al 2022](https://arxiv.org/abs/2205.09991)\n",
    "- Molecule Generation\n",
    "- DeepFake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19860f-56e0-45e2-929f-d6e6b371789a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Roadmap and Challenges in Generative Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e77e9-a003-41c6-b619-c373093d4e6e",
   "metadata": {},
   "source": [
    "**Representation**: how do we model the joint distribution of many random variables?\n",
    "\n",
    "**Learning** What is the right way to compare probability distributions?\n",
    "\n",
    "**Inference** How do we invert the generation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e91e2-8611-431d-b760-13d48329b93e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generative Model Curse of Dimensionality and Bayesian Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129020d4-d2da-4742-a2e1-b8ccd4c92463",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed7405-3224-46b7-968a-3b738bdd32a5",
   "metadata": {},
   "source": [
    "- What is a generative model\n",
    "- Representing probability distributions\n",
    "    - Curse of dimensionality\n",
    "    - Crash course on graphical models (Bayesian networks)\n",
    "    - Generative vs discriminative models\n",
    "    - Neural models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb3885-8aa5-4668-ac74-40ed41ac9004",
   "metadata": {},
   "source": [
    "#### Learning  a generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a15b9-863e-450e-994c-36df1af788d0",
   "metadata": {},
   "source": [
    "We want to learn a probability distribution $p(x)$ over images x such that\n",
    "\n",
    "**Generation** If we sample $x_{new} \\sim p(x)$,$x_{new}$ should look like a dog (sampling)\n",
    "\n",
    "**Density Estimation** p(x) should be high if x looks like a dog and low otherwise (anomly detection)\n",
    "\n",
    "**Unsupervised Representation Learning**: We should be able to learn what these images have in common, e.g. ears, tail, etc(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc30ac-de0c-457f-9187-3c3e66aea55e",
   "metadata": {},
   "source": [
    "#### How to Represent $p(x)$                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03cf144-313c-492c-9462-aa56469e8796",
   "metadata": {},
   "source": [
    "**Basic Discrete Distribution**\n",
    "- Bernoulli Distribution: (biased) coin flip\n",
    "    - $D = {Head,Tails}$\n",
    "    - $P(X=Heads) = p, P(X=Tails) = 1-p$\n",
    "    - $X \\sim Ber(p)$\n",
    "- Categorical Distribution: (biased) m-sided dice\n",
    "    - $D={1,...,m}$\n",
    "    - $P(Y=i) = p, \\sum p_i=1$\n",
    "    - $Y \\sim Cat(p_1,...,p_m)$\n",
    "\n",
    "Example of joint distribution: \n",
    "- Modeling pixels - Red, Blue, Green: $Val(R) = Val(B) = Val(G) = {0,...,255} = 256 * 256 * 256 - 1$ Number of parameters.\n",
    "- Modeling grey image numbers: Bernoulli $Val(X_i)={0,1} = 2^n -1 $ Number of parameters.\n",
    "\n",
    "$x-1$ number of parameters because the sum of $x$ parameters needs to sum up to 1; therefore, the last one is determined based on the previous $x-1$ parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b65766-81df-496b-88ed-a6b6a5e21864",
   "metadata": {},
   "source": [
    "**Assumption**: Independent\n",
    "$$p(x_1,...,x_n) = p(x_1)p(x_2)...p(x_n) $$\n",
    "\n",
    "- $2^n$ possible states\n",
    "- $p(x_1,...,x_n)$ we need only 1 parameters to specify marginal distribution $p(x_1)$\n",
    "- $2^n$ entries can be described by just n numbers (if $|Val(X_i)| = 2$).\n",
    "\n",
    "Problems: Too strong. Model may not be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa631a-2f7b-4cd8-853d-f9a610ae57bd",
   "metadata": {},
   "source": [
    "**Two Important Rules**\n",
    "- Chain Rule\n",
    "    - $P(S_1 \\cap S_2 \\cap \\cdots \\cap S_n) = p(S_1)p(S_2 | S_1) \\cdots p(S_n | S_1 \\cap \\cdots \\cap S_{n-1})$\n",
    "- Bayes' Rule\n",
    "    - $p(S_1 | S_2) = \\frac{p(S_1 \\cap S_2)}{p(S_2)} = \\frac{p(S_2 | S_1)p(S_1)}{p(S_2)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f2c00-f3ca-4f1e-9f22-a08bdc4d2adf",
   "metadata": {},
   "source": [
    "**Structure Through Conditional Independence**\n",
    "$$p(x_1,...,x_n) = p(x_1)p(x_2 |x_1)p(x_3 | x_1,x_2) \\cdots p(x_n | x_1,...,x_{n-1})$$\n",
    "\n",
    "- $p(x_1)$ requires 1 parameters.\n",
    "- $p(x_2 | x_1 = 0)$ requires 1 parameter, $p(x_2 | x_1 = 1)$ requires 1 parameter\n",
    "- In total, we need $1+2+ ... +2^{n-1}= 2^n-1$ parameters\n",
    "- It is still exponential.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e6e14-01ee-4996-a1d3-1e410b65302d",
   "metadata": {},
   "source": [
    "Now let's try, such as predicting next word.\n",
    "$$\\begin{align}\n",
    "p(x_1,...,x_n) & = p(x_1)p(x_2 |x_1)p(x_3 | x_1,x_2) \\cdots p(x_n | x_1,\\cdots,x_{n-1}) \\\\\n",
    "& = p(x_1) p(x_2 |x_1)p(x_3 | x_2)\\cdots p(x_n | x_{n-1})\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "It requires $2n-1$ parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313d571-30f6-488e-b1b1-3b83de0e5ecf",
   "metadata": {},
   "source": [
    "**Bayes Network** General Idea\n",
    "\n",
    "- Use conditional parameterization (instead of joint parameterization)\n",
    "- For each random variable $X_i$ specify $p(x_i |  \\mathbf{x_{A_i}})$ for set $\\mathbf{X_{A_i}}$ of random variables.\n",
    "\n",
    "$$p(x_1,\\cdots,x_n) = \\prod_i p(x_i |\\mathbf{x_{A_i}})$$\n",
    "\n",
    "- We need to guarantee it is a legal probability distribution.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a240a78-4e95-406c-830a-6efb7b6b4bac",
   "metadata": {},
   "source": [
    "**Bayesian Network** Formal\n",
    "\n",
    "A **Bayesian Network** is specified by a *directed* **acyclic** graph (DAG), $G=(V,E)$, with\n",
    "- One node $i \\in V$ for each random variable $X_i$\n",
    "- One conditional probability distribution (CPD) per node, $p(x_i |\\mathbf{x_{Pa_i}})$, specifying the variable's probability conditioned on its parents' value.\n",
    "\n",
    "Graph $G=(V,E)$ is called the structure of the Bayesian Network\n",
    "\n",
    "Defines a joint distribution:\n",
    "$$p(x_1, \\cdots, x_n) = \\prod_i p(x_i |\\mathbf{x_{Pa_i}})$$\n",
    "\n",
    "**Claim**: $p(x_1,...,x_n)$ is valid probability distribution because of ordering implied by DAG.\n",
    "\n",
    "**Economical Representation**: Exponential in $|Pa(i)|$, not |V|.\n",
    "$$p(x_1,\\cdots,x_n) = \\prod_i p(x_i |\\mathbf{x_{A_i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c894b-de91-4100-b711-ee84fe3e6b53",
   "metadata": {},
   "source": [
    "### Generative v.s. Discriminative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc38397-fc2d-4523-929b-2ae585fee266",
   "metadata": {},
   "source": [
    "**Naive Bayes for Single Label Prediction**\n",
    "\n",
    "- Words are conditionally independent given Y\n",
    "    - Let $1:n$ index the words in our vocabulary\n",
    "    - $X_i = 1$ if word $i$ appears in an email, and 0 otherwise\n",
    "    - E-mails are drawn according to some distribution $p(Y,X_1,\\cdots,x_n)$\n",
    "\n",
    "Then,\n",
    "$$p(y,x_1,\\cdots,x_n) = p(y) \\prod_{i=1}^n p(x_i |y)$$\n",
    "\n",
    "**Estimate** parameters from training data. **Predict** with Bayes rule:\n",
    "$$p(Y=1 | x_1,\\cdots,x_n) = \\frac{p(Y=1)\\prod_{i=1}^n p(x_i | Y = 1)}{\\sum_{y=\\{0,1\\}} p(Y=y) \\prod_{i=1}^n p(x_i | Y=y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2e18d-f997-4766-b7f3-71728fce601a",
   "metadata": {},
   "source": [
    "Chain Rule $p(Y,\\mathbf{X}) = p(\\mathbf{X} | Y) p(Y) = p(Y|\\mathbf{X}) p(\\mathbf{X})$\n",
    "\n",
    "Corresponding Bayesian Networks:<br>\n",
    "*Generative* $Y \\rightarrow X$ <br>\n",
    "*Discriminative* $X \\rightarrow Y$ <br>\n",
    "\n",
    "Suppose all we need for prediction is $p(Y|\\mathbf{X})$\n",
    "\n",
    "In the left model, we need to specify both $p(Y)$ and $p(\\mathbf{X}|Y)$, then compute $p(Y | \\mathbf{X})$ via the Bayes rule.\n",
    "\n",
    "In the right model, it suffices to estimate just the **conditional distribution** $p(Y|\\mathbf{X})$ \n",
    "- We never need to model/learn/use $p(\\mathbf{X})$!\n",
    "- Called a **discriminative** model because it is only useful for discriminating Y's label when given $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b9e8d-f8d1-4ab5-adfe-a44a1148498a",
   "metadata": {},
   "source": [
    "$$p(Y,\\mathbf{X}) = p(Y)p(X_1| Y )p(X_2 | Y,X_1) \\cdots p(X_n | Y,X_1,\\cdots,X_{n-1})$$\n",
    "$$p(Y,\\mathbf{X}) = p(X_1)p(X_2| X_1 )p(X_3 | X_1,X_2) \\cdots p(Y | X_1,\\cdots,X_{n-1},X_n)$$\n",
    "- In the generative model, $p(Y)$ is simple, but how do we parameterize $p(X_i | \\mathbf{X}_{pa(i)}, Y)$\n",
    "- In the discriminative model, how do we parameterize $p(Y | \\mathbf{X})$? Here we assume we don't care about modeling $p(\\mathbf{X})$ because  $\\mathbf{X}$ is always given to us in a classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a079057-4853-428e-9d69-2eb9d9ec7a82",
   "metadata": {},
   "source": [
    "**Naive Bayes**\n",
    "- For the generative model, assume that $X_i \\perp X_{-i} | Y$ **Naive Bayes**\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img height=\"100%\" width=\"50%\" src=\"sources/M1_1_6.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989442d-ee08-4223-8f0a-7d407980d2a5",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "Discriminative Model: $$p(Y=1|\\mathbf{x;\\alpha}) = f(\\mathbf{x},a)$$\n",
    "\n",
    "It is a parameterized function of x (regression). It has to be between 0 and 1.\n",
    "\n",
    "Linear Dependence: \n",
    "\n",
    "let $z(\\alpha;x)= \\alpha_0 + \\sum_{i=1}^n \\alpha_i x_i$. \n",
    "\n",
    "Then, $p(Y=1 | x;\\alpha) = \\sigma (z (\\alpha, x))$ where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is called **logistic function**.\n",
    "\n",
    "\n",
    "- Decision Boundary $p(Y=1 | x;\\alpha) > 0.5$ is linear in x\n",
    "- Equal Probability contours are straight lines.\n",
    "\n",
    "Logistic model does not assume $X_i \\perp X_{-i} | Y$. For example, in spam classification. Let $X_1$ = \"bank\" in email and $X_2$ = \"account\" in email. Assume that regardless of whether spam, these always appear together, i.e. $X_1 = X_2$.\n",
    "\n",
    "Learning in naive Bates results in $p(X_1 | Y) = p(X_2 | Y)$, Thus naive Bayes double counts the evidence.\n",
    "\n",
    "Using a conditional model is only possible when $X$ is observed. When some $X_i$ variables are unobserved, the generative model allow us to compute $p(Y | X_{evidence})$ by marginalizing over the unseen variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95fa15-1595-4db3-98d0-3e6340a176cd",
   "metadata": {},
   "source": [
    "**Logistic Regression** is stronger than **Naive Bayes** in practice because it make weaker assumptions. Therefore, if you have less data try Naive Bayes because it makes stronger assumptions, so there is no need with many data to figure out the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e2b4d-3fc1-43e8-8e7a-fef7abd26e3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Neural Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dce0b2-6498-45d6-9cb0-67ec9acb15a4",
   "metadata": {},
   "source": [
    "**None-linear dependence**: let $h(A,b,x) = f(Ax+b)$ be a non-linear transformation of the inputs (features).\n",
    "$$p_{Neural} (Y=1 | x;\\alpha,A,b) = \\sigma(\\alpha_0 + \\sum_{i=1}^h \\alpha_ih_i)$$\n",
    "- More Flexible\n",
    "- More Parameters: $A, b, \\alpha$\n",
    "- Can repeat multiple times to get a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d83d0-aa5c-43a5-aee1-26cd8248a8b6",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img height=\"100%\" width=\"50%\" src=\"sources/M1_1_7.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a840971-892b-403a-9636-9476c4589bee",
   "metadata": {},
   "source": [
    "**Continuous Variables**\n",
    "If X is a continuous random variable, we can use *probability density function* $p_X: \\mathbb{R} \\rightarrow \\mathbb{R}^+$. Typically, consider parameterized densities:\n",
    "- Gaussian: $X \\sim N(\\mu,\\sigma)$ if $p_X(x) = \\frac{1}{\\sigma \\sqrt{2[\\pi}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$\n",
    "- Uniform: $X \\sim \\mu(a,b)$ if $p_X(x)=\\frac{1}{b-a} 1[a\\leq x\\leq b]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34d7b1-b48c-484b-a9a8-e389c4504d53",
   "metadata": {},
   "source": [
    "If X is a continuous random vector, we can usually represent it using its **joint probability density function**.\n",
    "- Gaussian: if $p_X(x) = \\frac{1}{\\sqrt{(2\\pi})^n |\\sum|} exp(\\frac{-\\frac{1}{2}(x-\\mu)^T \\sum^{-1}(x-\\mu)})$\n",
    "\n",
    "Chain Rule, Bayes Rule still apply.\n",
    "$$p_{X,Y,Z}(x,y,z) = p_X(x)p_{Y|X}(y|x) p_{Z|\\{X,Y\\}}(z|x,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3edd491-2d73-44ae-8010-328d91229a4a",
   "metadata": {},
   "source": [
    "We can still use Bayesian Networks with continuous (and discrete) variables.\n",
    "\n",
    "For example: **Mixture of 2 Gaussians**: Bayes net $Z \\rightarrow X$ with factorization $p_{Z,X}(z,x) = p_Z(z)p_{X|Z}(x|z)$ and\n",
    "- $Z \\sim Bernoulli(p)$\n",
    "- $X|(Z=0) \\sim N(\\mu_0,\\sigma_0)$, $X|(Z=1)\\sim N(\\mu_0,\\sigma_0)$\n",
    "- The parameters are $p,\\mu_0,\\sigma_0,\\mu_1,\\sigma_1$\n",
    "  \n",
    "Bayes Net $Z \\rightarrow X$ with factorization  $p_{Z,X}(z,x) = p_Z(z)p_{X|Z}(x|z)$ and\n",
    "- $Z \\sim \\mu(a,b)$\n",
    "- $X|(Z=z) \\sim N(z,\\sigma)$\n",
    "- The parameters are $a,b,\\sigma$\n",
    "\n",
    "Variational Autoencoder: Bayes net $Z \\rightarrow X$ with factorization $p_{Z,X}(z,x) = p_Z(z)p_{X|Z}(x|z)$ and\n",
    "- $Z\\sim N(0,1)$\n",
    "- $X|(Z=z) \\sim N(\\mu_{\\theta}(z),e^{\\sigma_{\\theta}})$,where $\\mu_{\\theta}: \\mathbb{R} \\rightarrow \\mathbb{R}$ and $\\sigma_{\\phi}$ are neural networks with parameters (weights) $\\theta$,$\\phi$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4eaf47-8aeb-4221-9456-e93246d7f6fd",
   "metadata": {},
   "source": [
    "## Module 2 Autoregressive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f3848-454e-46ee-ac1f-48eb5866ec9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FVSBN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c79cd-a7fa-4fb7-a59a-407f23f0e12c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Neural Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97dd72-f5aa-4b30-9980-0a2694f43aad",
   "metadata": {},
   "source": [
    "**Setting**: \n",
    "\n",
    "Binary classification of $Y \\in \\{0,1\\}$ given input features $X \\in \\{0,1\\}^n$\n",
    "\n",
    "For classification, we care about $P(Y|x)$, and assume that $P(Y=1 | x;\\alpha) = f(x,\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab0cb9-2a42-4a09-a656-7aef6a3e7404",
   "metadata": {},
   "source": [
    "Motivating Example: MNIST\n",
    "\n",
    "**Given**: a dataset $D$ of handwritten digits, each image has n=28*28 = 784 pixels of black (0) and white (1)\n",
    "**Goal**: Learn a probability distribution $p(x) = p(x_1,\\cdots,x_784)$ over $x \\in \\{0,1\\}^{784}$ such that when $x \\sim p(x)$, x looks like a digit.\n",
    "\n",
    "Process:\n",
    "1. Parameterize a model family $\\{p_{\\theta}(x),\\theta \\in \\Theta\\}$\n",
    "2. Search for model parameters $\\theta$ based on training data $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c763178-6a23-4b88-bd6e-905b8dbb3615",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Autoregressive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe6103-bcaf-4e90-8dfa-72dadbef5361",
   "metadata": {},
   "source": [
    "We can pick an ordering of all the random variables, i.e. raster scan ordering of pixels from top-left (X_1) to bottom-right ($X_{n=784}$)\n",
    "\n",
    "$$p(x_1,\\cdots,x_{784}) = p(x_1)p(x_2| x_1) p(x_3 | x_1,x_2)\\cdots p(x_n|x_1,\\cdots,x_{n-1})$$\n",
    "\n",
    "Some conditional are too complex to be stored in tabular form. Instead, we assume \n",
    "$$p(x_1,\\cdots,x_{784}) = p_{CPT}(x_1;\\alpha^1) p_{logit}(x_2 | x_1;\\alpha^2) p_{logit}(x_3 | x_1,x_2;\\alpha^3)\\cdots p_{logit}(x_n | x_1,\\cdots,x_{n-1};\\alpha^n)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d063f-5c5c-44b0-8cb7-2aa0f228cfaa",
   "metadata": {},
   "source": [
    "More explicitly:\n",
    "\n",
    "$p_{CPT}(X_1=1;\\alpha^1) = \\alpha^1, p(X_1 = 0) = 1-\\alpha^1$\n",
    "\n",
    "$p_{logit}(X_2=1 | x_1;\\alpha^2)=\\sigma(\\alpha_0^2 + \\alpha_1^2x_1)$\n",
    "\n",
    "$p_{logit}(X_3=1 | x_1,x_2;\\alpha^3)=\\sigma(\\alpha_0^3 + \\alpha_1^3x_1,\\alpha_2^3x_2)$\n",
    "\n",
    "This is **modeling assumption**. Given all the previous ones, we are using parameterized functions to predict the next pixel. It might work well; it might not work well. It depends on how easy the relationship between two pixels is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952425a0-eb77-4206-bd1e-694cc35fb4ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Fully Visible Sigmoid Belief Network (FVSBN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b211010-806e-4ce5-a1c4-4b4ad515cf7b",
   "metadata": {},
   "source": [
    "The conditional variables $X_i | X_1,\\cdots,X_{i-1}$ are Bernoulli with parameters.\n",
    "$$\\hat{x}_i = p(X_i = 1 | x_1,\\cdots,x_{i-1};\\alpha^i) = p(X_i = 1 | x_{<i}; \\alpha^i) = \\sigma(\\alpha_0^i + \\sum_{j=1}^{i-1} \\alpha_j^ix_j)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba8499-e01f-4b6e-a432-2a93ff19f5c9",
   "metadata": {},
   "source": [
    "How to evaluate $p(x_1,\\cdots,x_{784})$?\n",
    "$$\\begin{align*}\n",
    "p(X_1 = 0,X_2 =1,X_3=1,X_4 = 1) & = (1-\\hat{x}_1) \\times \\hat{x}_2 \\times \\hat{x}_3 \\times (1-\\hat{x}_4)\\\\\n",
    "& = (1-\\hat{x}_1) \\times \\hat{x}_2(X_1 = 0) \\times \\hat{x}_3(X_1 = 0,X_2 = 1) \\times (1-\\hat{x}_4(X_1 = 0,X_2 = 1,X_3 = 1))\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c41dac0-5589-4e8a-aa9a-f1bc6c15db18",
   "metadata": {},
   "source": [
    "How to sample from $p(x_1,\\cdots,x_{784})$?\n",
    "\n",
    "- Sample $\\bar{x}_1 \\sim p(x_1)$ (np.random.choise([1,0]),p(\\hat{x}_1,1-\\hat{x}_1))\n",
    "- Sample $\\bar{x}_2 \\sim p(x_2 |x_1 = \\bar{x}_1 )$\n",
    "- Sample $\\bar{x}_3 \\sim p(x_3 |x_1 = \\bar{x}_1 ,x_2 = \\bar{x}_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48c30d-aedc-47fc-9b39-0e8d23bc7d47",
   "metadata": {},
   "source": [
    "The performance of this model is bad because logistic regression is not able to capture the relative relationship between pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed78028-a57b-4692-a8c1-669066414111",
   "metadata": {},
   "source": [
    "### NADE: Neural Autogressive Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1f19f-9cf4-4d19-81aa-8967bf2df965",
   "metadata": {},
   "source": [
    "To improve FBSBN model, use one layer neural network instead of logistic regression.\n",
    "\n",
    "$$h_i = \\sigma(A_i x_{<i}+c_i)$$\n",
    "$$\\hat{x}_i = p(x_i | x_1,\\cdots,x_{i-1};A_i,c_i,\\alpha_i,b_i) = \\sigma(\\alpha_ih_i+b_i)$$\n",
    "where $A_i,c_i,\\alpha_i,b_i$ are parameters\n",
    "\n",
    "For example $h_2 = \\sigma(A_2x_1+c_2), h_3 = \\sigma(A_3x_{1,2} + c_3)$ \n",
    "\n",
    "Tie weights to reduce the number of parameters and speed up computation.\n",
    "$$h_i = \\sigma(W_{\\cdot}  {<i}x_{<i}+c)$$\n",
    "$$\\hat{x}_i = p(x_i | x_1,\\cdots,x_{i-1}) = \\sigma(\\alpha_ih_i+b_i)$$\n",
    "where $A_i,c_i,\\alpha_i,b_i$ are parameters\n",
    "\n",
    "For examples:\n",
    "<div style=\"text-align:center;\">\n",
    "  <img height=\"100%\" width=\"50%\" src=\"sources/M2_1_2.png\" />\n",
    "</div>\n",
    "\n",
    "If $h_i \\in \\mathbb{R}^d$ How many parameters?\n",
    "- Linear in n: Weights $W \\in \\mathbb{R}^{d \\times n}$, biases $c \\in \\mathbb{R}^d$, n logistic regression coefficient vectors $\\alpha_i, b_i \\in \\mathbb{R}^{d+1}$\n",
    "- Probability is evaluated in O(nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b09ce3-5ffa-4f5a-892f-de7d00dd2b24",
   "metadata": {},
   "source": [
    "#### General Discrete Distributin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700401bc-e6cc-4c4a-8dd1-f4a42a3f95ac",
   "metadata": {},
   "source": [
    "How to model non-binary discrete random variables $X_i \\in \\{1,\\cdots, K\\}$\n",
    "- One Solution: Let $\\hat{x}_i$ parameterize a categorical distribution\n",
    "\n",
    "$$h_i = \\sigma(W_{\\cdot}  {<i}x_{<i}+c)$$\n",
    "$$p(x_i | x_1,\\cdots,x_{i-1}) = Category(p_i^1,\\cdots,p_i^K)$$\n",
    "$$\\hat{x}_i = (p_i^1,\\cdots p_i^K) = softmax(A_ih_i+b_i)$$\n",
    "\n",
    "Softmax generalizes the sigmoid/logistic function $\\sigma(\\cdot)$ and transforms a vector of K numbers into a vector of K probabilities (non-negative, sum to 1).\n",
    "\n",
    "$$softmax(a) = softmax(\\alpha^1,\\cdots,a^K) = (\\frac{exp(a^1)}{\\sum_i exp(a^i)},\\cdots,\\frac{exp(a^K)}{\\sum_i exp(a^i)})$$\n",
    "\n",
    "Python: \n",
    "> np.exp(a)/np.sum(np.exp(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf9b22-bf46-4ae7-ba1b-24e730d80c08",
   "metadata": {},
   "source": [
    "### RNADE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e311f-3dae-45b5-820f-254a7e420d61",
   "metadata": {},
   "source": [
    "How to model continuous random variables $X_i \\in \\mathbb{R}$? E.g. speech signals.\n",
    "\n",
    "Solution: Let $\\hat{x}_i$ parameterize a continuous distribution.\n",
    "\n",
    "$$p(x_i | x_1,\\cdots,x_{i-1}) = \\sum_{j=1}^K \\frac{1}{K} N(x_i;\\mu^j_i,\\sigma^i_i)$$\n",
    "\n",
    "$$h_i = \\sigma(W_{\\cdot}  {<i}x_{<i}+c)$$\n",
    "\n",
    "$$\\hat{x}_i = (\\mu_i^1,\\cdots,\\mu_i^K,\\sigma_i^1,\\cdots,\\sigma_i^K) = f(h_i)$$\n",
    "\n",
    "\n",
    "$$\\hat{x}_i$$ defines the mean and standard deviation of each of the K Gaussians $$(\\mu_i^j, \\sigma_i^j)$$\n",
    "\n",
    "Can use exponential $exp(\\cdot)$ to ensure non-negativity\n",
    "\n",
    "E.g. uniform mixture of K Gaussians.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c8eaf-69f7-4f51-80f4-3db1d4e5cbce",
   "metadata": {},
   "source": [
    "### Autoregressive Models vs Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97e521-f556-4f4b-92ca-1d31a70dfc96",
   "metadata": {},
   "source": [
    "FVSBN and NADE look similar to **Autoencoder**\n",
    "- an encoer $e(\\cdot)$, E.g. $e(x) = \\sigma(W^2 (W^1x +b^1)+b^2)$\n",
    "- an decoder such that $d(e(x)) \\approx x.$ E.g. $d(h) = \\sigma(Vh+c)$\n",
    "- Loss function for dataset D\n",
    "    - Binary: $\\min_{W^1,W^2,b^1,b^2,VC} \\sum_{x \\in D} \\sum_{i} -x_i log \\hat{x}_i -(1-x_i) log(1-\\hat{x}_i)$\n",
    "    - Continuous: $\\min_{W^1,W^2,b^1,b^2,V} \\sum_{x \\in D} \\sum_{i} (x_i - \\hat{x}_i)^2$\n",
    "- e and d are constrained so that we don't learn identity mappings. Hope that $e(x)$ is a meaningful, compressed representation of x (feature learning)\n",
    "- A vanilla autoencoder is *not* a generative model: it does not define a distribution over x we can sample from to generate new data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ac963-2957-49c1-9a5d-d68a66ce1487",
   "metadata": {},
   "source": [
    "We need to ensure it corresponds to a valid Bayesian Network (DAG structure), i.e. we need an ordering for chain rule. If ordering is 1,2,3, then:\n",
    "- $\\hat{x}_1$ cannot depend on any input $x=(x_1,x_2,x_3)$; then, at generation time, we don't need any input to get started.\n",
    "- $\\hat{x}_2$ can only depend on $x_1$\n",
    "\n",
    "Bonus: we can use a single neural network (with n inputs and outputs to produce all parameters $\\hat{x}$ in a single pass). In contrast, NADE requires n passes. Much more efficient on modern one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b3576-ce09-41dd-82ae-6c0ac51e6e09",
   "metadata": {},
   "source": [
    "#### MADE: Masked Autoencoder for Distribution Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66251dd5-50d5-4717-a7e4-b5907fe09b7a",
   "metadata": {},
   "source": [
    "**Challenge**: An autoencoder that is autoregressive (DAG structure)\n",
    "\n",
    "**Solution**: use masks to disallow certain path (Germain et al 2015). Suppose ordering is $x_2,x_3,x_1$ so $p(x_1,x_2,x_3) = p(x_2)p(x_3|x_2)p(x_1 |x_2,x_3)$\n",
    "- The unit producing the paramters for $\\hat{x}_2 = p(x_2)$ is not allowed to depend on any input.\n",
    "- For each unit in a hidden layer, pick a random integer $i$ in $[1,n-1]$. That unit is allowed to depend only on the first $i$ inputs (according to the chosen ordering)\n",
    "- Add a mask to preserve this invariant: connect to all units in the previous layer with a smaller or equal assigned number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343c484-1aba-4cc5-9ef8-697d73dac01f",
   "metadata": {},
   "source": [
    "#### RNN: Recurrent Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd254afa-b837-40d1-998f-043a3537e0d2",
   "metadata": {},
   "source": [
    "**Challenge**: model p(x_t | x_{1:t-1};\\alpha^t). \"History\" x_{1:t-1} keeps getting longer.\n",
    "\n",
    "**Idea**: keep a summary and recursively update it\n",
    "<div style=\"text-align:center;\">\n",
    "  <img height=\"100%\" width=\"50%\" src=\"sources/M2_1_3.png\" />\n",
    "</div>\n",
    "\n",
    "- Summary update rule $h_{t+1}$ = tanh(W_{hh}h_t + W_{xh}x_{t+1})\n",
    "- Prediction $o_{t+1} = W_{hy} h_{t+1}$\n",
    "- Summary initalization: $h_0 = b_0$\n",
    "\n",
    "Hidden Layer $h_t$ is a summary of the inputs seen till time t\n",
    "\n",
    "Output layer $o_{t-1}$ specifies parameters for conditional $p(x_t | x_{1:t-1})$\n",
    "\n",
    "Parameterized by $b_0$ (initialization), and matrices $W_{hh},W_{xh}, W_{hy}$. Constant number of parameters with regard to n!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb99a0-59ec-4a35-8411-c50d031bf22d",
   "metadata": {},
   "source": [
    "**Example: Character RNN (from Andrej Karpathy)**\n",
    "\n",
    "- Use one-hot encoding for $x_i \\in \\{h,e,l,o\\}$\n",
    "- **Autoregressive**: $p(x = hello) = p(x_1 = h)p(x_2 = e|x_1 = h)p(x_3 = l | x_1 = h,x_2 = 3), \\cdots p(x_5 = o | x_1 = h,x_2 = e,x_3 = l,x_4 = l)$\n",
    "\n",
    "$$p(x_2 = e | x_1 = h) = softmax(o1) = \\frac{exp(2.2)}{exp(1.0) + \\cdots + exp(4.1)}$$\n",
    "$$o_1 = W_{hy}h_1$$\n",
    "$$h_1 = tanh(W_{hh}h_0 + W_{xh}x_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce2187-b2f3-487b-b277-05c6aa6fa19e",
   "metadata": {},
   "source": [
    "**Pros**\n",
    "- Can be applied to sequences of arbitrary length\n",
    "- Very general: For every computable function, there exists a finite RNN that can compute it\n",
    "\n",
    "**Cons**\n",
    "- Still requires an ordering\n",
    "- Sequential likelihood evaluation (very slow for training)\n",
    "- Sequential generation (unavoidable in an autoregressive model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a665c-ca46-4ce0-8fb3-97f4b510b41f",
   "metadata": {},
   "source": [
    "**Issue with RNN models**\n",
    "- A single hidden vector needs to summarize all the (growing) history. For example $h^4$ needs to be summarized the meaning of \"My friend opened the\"\n",
    "- Sequential Evaluation, cannot be parallelized\n",
    "- Exploding/Vanishing gradients when accessing information from many steps back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0254e8-37bb-49c0-9a96-d6a2f59ed16c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attention-based Models vs RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298724c-7872-436f-acac-48460531e77f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Attention based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f67256-2c04-470a-92d4-a4d29ddeb59a",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img height=\"50%\" width=\"50%\" src=\"sources/M2_1_4.png\" />\n",
    "</div>\n",
    "\n",
    "Attention mechanism to compare a *query* vector to a set of *key* vectors.\n",
    "- Compare current hidden state (*query*) to all past hidden states (*keys*)\n",
    "- Construct attention distribution to figure out what parts of the history are relevant, e.g. via a softmax\n",
    "- Construct a summary of the history, e.g. by weighted sum\n",
    "- Use summary and current hidden state to predict the next token/word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344430ea-be08-4496-aee9-00f8e537656d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Generative Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdcd32d-7ed2-4130-9961-c9bed53d2869",
   "metadata": {},
   "source": [
    "Current state of the art (GPTs): replace RNN with Transformer\n",
    "- Attention mechanisms to adaptively focus only on relevant context\n",
    "- Avoid recursive computation. Use only self-attention to enable parallelization\n",
    "- Needs **masked** self-attention to preserve autoregressive structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b3881-002d-4a70-89ae-407571906e69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Pixel RNN (Oord et al 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7ec16-7556-47e1-9d47-40a88a5110ca",
   "metadata": {},
   "source": [
    "Model images pixel by pixel using raster scan order.\n",
    "\n",
    "Each pixel conditional $p(x_t | x_{1:t-1})$ needs to specify 3 color.\n",
    "$$p(x_t | x_{1:t-1}) = p(x_t^{red} | x_{1:t-1} )p(x_t^{green} | x_{1:t-1};x_t^{red} )p(x_t^{blue} | x_{1:t-1} ;x_t^{red};x_t^{green})$$\n",
    "\n",
    "and each conditional is a categorical random variable with 256 possible.\n",
    "\n",
    "Conditionals modeled using RNN variants. LSTMs + masking (like MADE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec45b6-7d90-4fcb-9730-4bfef55a1b41",
   "metadata": {},
   "source": [
    "#### Convolutional Architectures - Pixel CNN (Oord et al 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0395bf9-2e13-4a8b-bac7-0eb84007acae",
   "metadata": {},
   "source": [
    "**Idea**: Use convolutional architecture to predict next pixel given context (a neighborhood of pixels).\n",
    "\n",
    "**Challenge**: Has to be autoregressive. Masked convolutions preserve raster scan order. Additional masking for colors order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2687c33-ecc0-4c82-b89b-37c7a33386ee",
   "metadata": {},
   "source": [
    "#### Application in Adversarial Attacks and Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e48f2-5c5c-4287-ae57-6f55a113e7c3",
   "metadata": {},
   "source": [
    "Machine Learning methods are vulnerable to adversarial examples.\n",
    "\n",
    "dog + noise = ostrich\n",
    "\n",
    "**PixelDefend (Song et al 2018)**\n",
    "- Train a generative model $p(x)$ on clean inputs (PixelCNN)\n",
    "- Given a new input $\\bar{x}$, evaluate $p(\\bar{x})$\n",
    "- Adversarial examples are significantly less likely under $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb397162-f380-4667-b678-af3577f73e00",
   "metadata": {},
   "source": [
    "#### Summary of Autoregressive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d25cef-bbae-48de-8ae6-14638df8a6dc",
   "metadata": {},
   "source": [
    "Easy to sample from:\n",
    "1. Sample $\\bar{x}_0 \\sim p(x_0)$\n",
    "2. Sample   $\\bar{x}_1 \\sim p(x_1 | x_0 =\\bar{x}_0)$\n",
    "\n",
    "Easy to compute probability $p(x=\\bar{x})$\n",
    "1. Compute $p(x_0 = \\bar{x}_0)$\n",
    "2. Compute $p(x_0 = \\bar{x}_1 | x_0 = \\bar{x}_0)$\n",
    "3. Multiply together (sum their logarithms)\n",
    "4. ...\n",
    "5. Ideally, can compute all these terms in parallel for fast training\n",
    "\n",
    "Easy to extend to continuous variable. For example, can choose Gaussian conditionals $p(x_t | x_{<t}) = N(\\mu_{\\theta}(x_{<t}),\\sum_{\\theta}(x_{<t}))$ or mixture of logistics\n",
    "\n",
    "No natural way to get features, cluster points, do unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ce05e-53c4-4995-9068-6adf31df508f",
   "metadata": {},
   "source": [
    "### KL Divergence - Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d65f07-00dc-45aa-beb2-21eaea94d0c1",
   "metadata": {},
   "source": [
    "**Goal of Learning**:  return a model $P_{\\theta}$ that precisely captures the distribution $P_{data}$ from which our data was sampled.\n",
    "\n",
    "What is **\"best\"**?\n",
    "- Density Estimation: we are interested in the full distribution (so later we can compute whatever conditional probabilities we want)\n",
    "- Specific prediction tasks: we are using the distribution to make a prediction\n",
    "    - **Structured prediction**: Predict next frame in a video, or caption given an image\n",
    "-  Structure or Knowledge Discovery: we are interested in the model itself\n",
    "    -   How do some genes interact with each other?\n",
    "    -   What causes cancer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622a8ad-cdd6-4dad-a07f-5c6f8f428c2b",
   "metadata": {},
   "source": [
    "#### Learning as **Density Estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a68a5-c1e1-453d-88d7-8646015a2d23",
   "metadata": {},
   "source": [
    "We want to construct $P_{\\theta}$ as \"close\" as possible to $P_{data}$ (recall we assume we are given a dataset $D$ of samples from $P_{data}$)\n",
    "\n",
    "How do we evaluate \"closeness\"?\n",
    "- [KL-Divergence](#KL-Divergence)\n",
    "$$D(P_{data} || P_{\\theta}) = E_{x\\sim p}[log \\frac{P_{data}(x)}{P_{\\theta}(x)}]$$\n",
    "\n",
    "$D(P_{data} || P_{\\theta}) = 0$ iff the two distributions are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9204d-deb8-4f4b-af14-1b5b7c1f5162",
   "metadata": {},
   "source": [
    "#### KL-Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b25282-574a-4ac2-9225-8a0f2aa3f2b4",
   "metadata": {},
   "source": [
    "We use KL-Divergence to measure distance between distribution.\n",
    "\n",
    "**Kullback-Leibler Divergence** (KL-divergence) between two distribution $p$ and $q$ is defined as\n",
    "$$D(p ||q) = \\sum_x p(x)log \\frac{p(x)}{q(x)}$$\n",
    "\n",
    "$$D(p || q ) \\geq 0$$ for all p,q, with equality if and only if $p=q$. It is non-negative.\n",
    "\n",
    "Proof:\n",
    "$$E_{x\\sim p}[-log \\frac{q(x)}{p(x)}] \\geq -log(E_{x\\sim p}[\\frac{q(x)}{p(x)})]) = - log (\\sum_x p(x)\\frac{q(x)}{p(x)}) = 0$$\n",
    "\n",
    "Note that KL-divergence is **asymmetric**, i.e. $D(p || q ) \\neq D(q||p)$\n",
    "\n",
    "Measures the expected number of extra bits required to describe samples from $p(x)$ using a compression code based on $q$ instead of $p$\n",
    "- If your data comes from $p$, but you use a scheme optimized for $q$, the divergence $D_{KL}(p||q)$ is the number of extra bits you'll need on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2060149f-e9fd-4760-b037-1d783d38d5af",
   "metadata": {},
   "source": [
    "#### Expected Log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44e18e-3a6c-4a68-bdf9-ccab81f22343",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "D(P_{data} || P_{\\theta}) & = E_{x\\sim p}[log \\frac{P_{data}(x)}{P_{\\theta}(x)}]\\\\\n",
    "& = E_{x\\sim p}[log P_{data}(x)]-E_{x\\sim p}[log P_{\\theta}(x)]\n",
    "\\end{align*}$$\n",
    "\n",
    "The first term does not depend on $P_{\\theta}$\n",
    "\n",
    "Then, *minimizing* KL divergence is equivalent to *maximizing* the **expected log-likelihood**\n",
    "$$argmin_{P_{\\theta}}D(P_{data} || P_{\\theta}) = argmin_{P_{\\theta}} -E_{x\\sim p}[log P_{\\theta}(x)] = argmax_{P_{\\theta}} E_{x\\sim p}[log P_{\\theta}(x)]$$\n",
    "- $P_{\\theta}$ assign high probability to instances sampled from $P_{data}$ so as to reflect the true distribution\n",
    "- Because of log, samples x where $P_{\\theta}(x) \\approx 0$ weigh heavily in objective\n",
    "- **Problem**: Although we can not compare models since we are ignoring $H(P_{data}) = - E_{x\\sim p}[log P_{data}(x)]$ , we don't know how close we are to the optimum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14563c3-9d94-4ad8-aea1-bd390cc4c3d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0555c0-ef7c-463d-a4d4-832f57469a06",
   "metadata": {},
   "source": [
    "Approximate the expected log-likelihood $$E_{x\\sim p}[log P_{data}(x)]$$\n",
    "with the empirical log-likelihood:\n",
    "$$E_{D}[log P_{\\theta}(x)] = \\frac{1}{D}\\sum_{x\\in D}log P_{\\theta}(x)$$\n",
    "\n",
    "Maximum likelihood learning is then:\n",
    "$$max_{P_{\\theta}} \\frac{1}{|D|}\\sum_{x \\in D} log P_{\\theta}(x)$$\n",
    "\n",
    "Equivalently, maximize likelihood of the data\n",
    "$$P_{\\theta}(x^1,\\cdots,x^m) = \\prod_{x \\in D} P_{\\theta}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57925954-601e-41f7-beaa-cbc47be1fc92",
   "metadata": {},
   "source": [
    "### Monte Carlo Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e1af72-8188-4a16-8caf-e22e76af672c",
   "metadata": {},
   "source": [
    "#### Main idea in Monte Carlo Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62131d89-0b5c-4e27-819a-10069b8d8a17",
   "metadata": {},
   "source": [
    "Express the quantity of interest as the expected value of a random variable\n",
    "$$E_{x\\sim P}[g(x)] = \\sum_x g(x)P(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c46d9-6744-485c-a966-7e5e24bad104",
   "metadata": {},
   "source": [
    "Alternatively, Generate T samples $x^1,\\cdots x^T$ from the distribution P with respect to which the expectation was taken.\n",
    "\n",
    "Estimate the expected value from the samples using:\n",
    "$$\\hat{g}(x^1,\\cdots x^T) = \\frac{1}{T}\\sum_{t=1}^T g(x^t)$$\n",
    "where $x^1,\\cdots x^T$ are independent samples from P."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43192f-4a20-47bf-8c12-6c9c26995129",
   "metadata": {},
   "source": [
    "#### Properties of Monte Carlo Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74735b73-9b09-40ee-977c-e8f6a590f3a8",
   "metadata": {},
   "source": [
    "- **Unbiased**\n",
    "  $$E_{P}[\\hat{g}] = E_P[g(x)]$$\n",
    "- **Convergence**: By law of large numbers\n",
    "  $$\\hat{g} =  \\frac{1}{T}\\sum_{t=1}^T g(x^t) \\rightarrow E_P[g(x)] \\text{ for } T \\rightarrow \\infty$$ \n",
    "- **Variance**\n",
    "  $$V_P[\\hat{g} ] = V_P [\\frac{1}{T}\\sum_{t=1}^T g(x^t)] = \\frac{V_P [g(x)]}{T}$$\n",
    "\n",
    "Thus, the variance of the estimator can be reduced by increasing the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f8402-fdad-4add-85cd-10f54c57ab99",
   "metadata": {},
   "source": [
    "#### Expanding the MLE principle to autoregressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba82eda-c028-46cc-aecf-3671a6d7777e",
   "metadata": {},
   "source": [
    "Given an autoregressive model with $n$ variables and factorization\n",
    "$$P_{\\theta}(x) = \\prod_{i=1}^n p_{neural}(x_i | x_{<i}; \\theta_i)$$\n",
    "\n",
    "$\\theta = (\\theta_1,\\cdots,\\theta_n)$ are parameters of all the conditionals.\n",
    "\n",
    "Training data $D={x^1,\\cdots,x^m}$. Maximum likelihood estimate of the parameters $\\theta$\n",
    "- Decomposition of likelihood function\n",
    "$$L(\\theta,D) = \\prod_{j=1}^m P_{\\theta}(x^j) = \\prod_{j=1}^m \\prod_{i=1}^n p_{neural} (x_i^j | x^i_{<i};\\theta_i)$$\n",
    "\n",
    "- Goal: maximize $argmax_{\\theta} L(\\theta,D) = argmax_{\\theta} log L(\\theta,D)$\n",
    "- We no longer have a closed form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8335c5d-a728-4354-97c2-6f6df7e4a101",
   "metadata": {},
   "source": [
    "#### MLE Learning: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4024ad-caf0-401d-b95b-ae56a6c7027f",
   "metadata": {},
   "source": [
    "$$L(\\theta,D) = \\prod_{j=1}^m P_{\\theta}(x^j) = \\prod_{j=1}^m \\prod_{i=1}^n p_{neural} (x_i^j | x^i_{<i};\\theta_i)$$\n",
    "\n",
    "Goal: maximize $argmax_{\\theta} L(\\theta,D) = argmax_{\\theta} log L(\\theta,D)$\n",
    "\n",
    "$l(\\theta) = log L(\\theta,D) = \\sum_{j=1}^m \\sum_{i=1}^n log p_{neural} (x_i^j | x^i_{<i};\\theta_i)$\n",
    "- Initialize $\\theta^0 = (\\theta_1,\\cdots,\\theta_n)$ at random\n",
    "- Compute $\\nabla_\\theta l(\\theta)$ (by back propagation)\n",
    "- $\\theta^{t+1} = \\theta^t + \\alpha_t \\nabla_\\theta l(\\theta)$\n",
    "\n",
    "Non-convex optimization problem, but often works well in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a863ee7-f7c2-4121-9f2c-32513edb532e",
   "metadata": {},
   "source": [
    "What is the gradient with respect to $\\theta_i$?\n",
    "\n",
    "$\\nabla_{\\theta_i} l(\\theta) =\\sum_{j=1}^m \\nabla_{\\theta_i} \\sum_{i=1}^n log p_{neural} (x_i^j | x^i_{<i};\\theta_i) =\\sum_{j=1}^m \\nabla_{\\theta_i} log p_{neural} (x_i^j | x^i_{<i};\\theta_i)$\n",
    "\n",
    "Each conditional $p_{neural}(x_i | x_{<i};\\theta_i)$ can be optimized separately if there is no parameter sharing.\n",
    "\n",
    "$\\nabla_{\\theta} l(\\theta)= \\sum_{j=1}^m \\sum_{i=1}^n \\nabla_{\\theta}  log p_{neural} (x_i^j | x^i_{<i};\\theta_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136f579-1bc8-40cf-8f01-df5f4fcb5f78",
   "metadata": {},
   "source": [
    "What if $m=|D|$ is huge?\n",
    "\n",
    "$\\begin{align}\n",
    "\\nabla_{\\theta} l(\\theta) &= m \\sum_{j=1}^m \\frac{1}{m} \\sum_{i=1}^n \\nabla_{\\theta}  log p_{neural} (x_i^j | x^i_{<i};\\theta_i) \\\\\n",
    "& = m E_{x^j \\sim D} [\\sum_{i=1}^n \\nabla_{\\theta}  log p_{neural} (x_i^j | x^i_{<i};\\theta_i)]\n",
    "\\end{align}$\n",
    "\n",
    "A uniform distribution over dataset\n",
    "\n",
    "Monte Carlo: Sample $x^j \\sim D$; $\\nabla_{\\theta} l(\\theta) =m [\\sum_{i=1}^n \\nabla_{\\theta}  log p_{neural} (x_i^j | x^i_{<i};\\theta_i)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05c21f-7100-460c-aecc-8ec724117c40",
   "metadata": {},
   "source": [
    "#### Empirical Risk and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde010a1-cd68-49f8-8dcc-7698c65a68dc",
   "metadata": {},
   "source": [
    "Empirical risk minimization can easily **overfit** the data\n",
    "\n",
    "**Generalization**: the data is a sample, usually there is vast amount of samples that you have never seen. Your model should generalize well to these \"never-seen\" sample.\n",
    "\n",
    "Thus we typically restrict the **hypothesis space** of distributions that we search over."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383fcd5-d83f-4b06-906e-94ca2f56498e",
   "metadata": {},
   "source": [
    "**Bias-Variance Trade Off**\n",
    "- If the hypothesis space is very limited, it might not be able to represent $P_{data}$ even with unlimited data\n",
    "    - This type of limitation is called **bias**, as the learning is limited on how close it can approximate the target distribution \n",
    "- If we select a highly expressive hypothesis class, we might represent better the data\n",
    "    - When we have small amount of data, multiple models can fit well. or even better than the true model.Moreover, small perturbations on $D$ will result in very different estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f2b86-5d42-4618-80d0-76a746fa66c4",
   "metadata": {},
   "source": [
    "#### How to avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223da50-90f5-428e-bfec-85017554ef94",
   "metadata": {},
   "source": [
    "- Hard constraints:\n",
    "    - smaller neural networks with less parameters\n",
    "    - weight sharing \n",
    "- Soft preference for \"simpler\"models\n",
    "- Augment the objective functions with regularizations\n",
    "  $$objective(x,M) = loss(x,M)+R(M)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
