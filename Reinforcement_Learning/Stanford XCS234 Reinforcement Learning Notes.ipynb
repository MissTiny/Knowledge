{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80761663-dbf4-42b3-b60d-7ccbb597cb1f",
   "metadata": {},
   "source": [
    "This is summary note for [Stanford XCS234 Reinforcement Learning](https://web.stanford.edu/class/cs234/). It contains some of my understanding and inference from learning process, while most of the notes are directly from the notes in course or the book [Reinforcement Learning, Sutton and Barto ](http://incompleteideas.net/book/the-book-2nd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d80aad-fa38-4864-a1b7-a0375864243b",
   "metadata": {},
   "source": [
    "# Module 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a04569-38b1-428a-8f00-efd403219718",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Application of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad0658-1ec8-4355-a81d-2c908344510e",
   "metadata": {},
   "source": [
    "**Reading Material:**\n",
    "- Ch.1 of **[Reinforcement Learning, Sutton and Barto ](http://incompleteideas.net/book/the-book-2nd.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6261d-93e0-4519-a65b-df685eafe2ec",
   "metadata": {},
   "source": [
    "**Reinforcement Learning** learns through experience or data to make good decisions under uncertainty. A learning **agent** must be able to sense the **state** of its **enviornment** to some extent and take **actions** to affect the state with a **goal** or goals related to the state of enviornment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e4826-e320-4158-86b0-dacf9a187e32",
   "metadata": {},
   "source": [
    "**Application**\n",
    "- Chess/Game: Go\n",
    "- Fusion Science: Learning Plasma Control\n",
    "- Covid Testing: Efficient and Targeted\n",
    "- ChatGPT\n",
    "    - Step 1: behavior Cloning, Imitation Learning\n",
    "    - Step 2: Model of Reward, Model-based RL\n",
    "    - Step 3: Reinforcement Learning, RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf7f0a-ce0e-49b7-8eed-d89902ecadcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reinforcement Learning Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d59caa-0041-477b-ace5-fce118f4d3db",
   "metadata": {},
   "source": [
    "- Optimization: Find the optimal way to make decisions\n",
    "- Delayed Consequences: Decisions now can impact things later; temporal credit is hard.\n",
    "- Exploration: Learning about the world by making decisions; You only get the result of what you try.\n",
    "- Generalization: Policy is mapping from past experience to action\n",
    "    - pre-program policies might be too large to cover all possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bb8d1-1ed7-4a0a-86b9-bc73b453a131",
   "metadata": {},
   "source": [
    "**Imitation Learning (IL)** assumes input demonstrations of good policies. It allows us to reducec RL to Supervised Learning (SL). For example, instead of code a policy for auto-driving, we can use data that has human driving it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f170fe-ff4b-4ecd-804d-4880a290d89d",
   "metadata": {},
   "source": [
    "**When RL is powerful**\n",
    "- No examples of desired behavior e.g. beyond human performance or no existing data\n",
    "- Enormous search or optimization problem with delayed outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e0673-9fd9-4691-9ca6-81caa093768f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reinforcement Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc6f17-7645-4455-b032-0f9475dd5b40",
   "metadata": {},
   "source": [
    "- State\n",
    "- Actions/Decisions\n",
    "- Reward model\n",
    "- Meaning of dynamics model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260269de-bc91-4991-88a7-47a2595edf3a",
   "metadata": {},
   "source": [
    "**Warning**: Reward Hacking - Choosing the easiest solution to maximize the reward but not meaningful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "160304ee-bc33-4d71-986c-717317d3b220",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img height=\"100%\" width=\"50%\" src=\"sources/XCS224_m1_p4_0.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af031fe-bd3c-4841-9421-deeb437ed2f4",
   "metadata": {},
   "source": [
    "**Two Object**\n",
    "- World/Environment\n",
    "- Agent: Our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e983b7-2bda-4667-ac83-e90ba3bbdde3",
   "metadata": {},
   "source": [
    "**Four Sub-element**\n",
    "- **policy**: It defines an agent's behavior at a given time and maps the state of the environment to actions.\n",
    "- **reward signal** (immediate): It defines the goal of a reinforcement learning problem. At each timestep, a reward (scalar value) is sent to an agent from the environment. A large number indicates good **actions**, and a small number indicates bad actions. If the actions are bad, the policy may be changed to select other actions next time. **Agent** aims to maximize its total rewards over the long run. \n",
    "- **value function**: it specifies what is good in the long run. A value of a **state** is the total amount of reward an agent can expect to accumulate over the future, starting from the current stage **(future expectations)**\n",
    "- **model of environment**: It is used to mimic the behavior of environment. The next state and next reward can be inferenced by model. **Model-based methods** are algorithms that use models of the environment where, whereas **Model-free methods** are simpler trial-and-error learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0980703-c928-4e3e-a4d7-3e3c2ddc8a97",
   "metadata": {},
   "source": [
    "## Self-Summary:\n",
    "\n",
    "1. What's relationship between reward and value?\n",
    "> The **Value** of a state is the expectation of future **rewards**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202dc5ed-c6c9-4350-85f6-1b8fb3cc2351",
   "metadata": {},
   "source": [
    "# Module 2: Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b29114-0b0f-4819-b1ac-62f04ec746ff",
   "metadata": {},
   "source": [
    "**Reading Material:**\n",
    "- Ch.3,4.1-4.4,5.1-5.5,6.1-6.7 of **[Reinforcement Learning, Sutton and Barto ](http://incompleteideas.net/book/the-book-2nd.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db280a54-4474-492f-a48c-cd1b4fb8e470",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e3439-f9cf-45d4-a132-e2215651100f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Markov Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10034c7-50db-4781-b995-4e28eebc19cb",
   "metadata": {},
   "source": [
    "**Markov Assumption**\n",
    "\n",
    "State $s_t$ is Markov iff: $p(s_{t+1} | s_t,a_t) = p(s_{t+1} | h_t,a_t)$, which means future is independent of past given present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ba836-2a17-4a8d-8c7d-36af7f69fd16",
   "metadata": {},
   "source": [
    "**Things to Think of**\n",
    "1. Is state Markov? Is world partially observable?\n",
    "> If the data can not help us identify the state, such as which floor I am based on razer, then it is partially observable.\n",
    "2. Are dynamics deterministic or stochastic?\n",
    "3. Do actions influence only immediate reward (bandits) or reward and next state?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697756eb-0f45-4e60-8b4e-ab1d17ca2a06",
   "metadata": {},
   "source": [
    "**MDP Model**\n",
    "Dynamic model predicts next agent state. \n",
    "$$p(s_{t+1} = s' | s_t =s, a_t = a)$$\n",
    "\n",
    "Reward model predicts immediate reward.\n",
    "$$r(s_t=s,a_t=a) = \\mathbb{E}[r_t|s_t=s,a_t=a]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd3a8845-61db-44e2-a456-b8e3185b3451",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    " <img height=\"100%\" width=\"50%\" src=\"sources/XCS224_2_1_1.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1f63b-e31b-46f5-8f1f-f01a0fa7e3c7",
   "metadata": {},
   "source": [
    "**Policy**\n",
    "\n",
    "Policy $\\pi$ determines how the agent chooses actions.\n",
    "\n",
    "$\\pi:S \\rightarrow A$ mapping from states to actions\n",
    "\n",
    "*Deterministic policy* $$\\pi(s)=a$$\n",
    "\n",
    "*Stochastic Policy* $$\\pi(a | s) = Pr(a_t = a | s_t = s)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8948ed-d371-4757-ba5b-2bb61c8ce904",
   "metadata": {},
   "source": [
    "**Evaluation and Control**\n",
    "\n",
    "*Evaluation* Estimate/predict the expected rewards from following a given policy\n",
    "\n",
    "*Control* Optimization: find the best policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080d1ce-804f-4466-a8ee-335bbf2d2257",
   "metadata": {},
   "source": [
    "**Markov Process or Markov Chain**\n",
    "\n",
    "*Memoryless random process* Sequence of random states with Markov property\n",
    "\n",
    "*Definition of Markov Process*\n",
    "> S is a (finite) set of states ($s \\in S$) <br>\n",
    "> P is dynamics/traistion model that specifices $p(s_{t+1} = s' | s_t = s)$\n",
    "\n",
    "Note, no rewards, no actions.\n",
    "\n",
    "If finite number (N) of states, can express P as a matrix\n",
    "\n",
    "$$p=\\begin{bmatrix} P(s_1|s_1) & P(s_2|s_1) & \\cdots & P(s_N | s_1) \\\\  P(s_2|s_1) & P(s_2|s_2) & \\cdots & P(s_N | s_2) \\\\ \\vdots & \\vdots & & \\vdots \\\\ P(s_1|s_N) & P(s_2|s_N) & \\cdots & P(s_N|s_N) \\end{bmatrix}$$\n",
    "\n",
    "All rows need to sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed9d6f-24b4-4baf-b433-653253d960e6",
   "metadata": {},
   "source": [
    "**Markov Reward Process (MRP)**\n",
    "\n",
    "*Markov Reward Process* = Markov Chain + rewards\n",
    "\n",
    "*Definition of Markov Process*\n",
    "> S is a (finite) set of states ($s \\in S$) <br>\n",
    "> P is dynamics/traistion model that specifices $p(s_{t+1} = s' | s_t = s)$<br>\n",
    "> R is a reward function $R(s_t = s) = \\mathbb{E}[r_t | s_t = s]$<br>\n",
    "> Discount factor $\\gamma \\in [0,1]$\n",
    "\n",
    "Note: No Action\n",
    "\n",
    "If finite number (N) of states can express R as a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18f694-0805-4fc5-bb66-36e88c199e49",
   "metadata": {},
   "source": [
    "**Return & Value Function**\n",
    "\n",
    "*Definition of Horizon (H)*\n",
    "> Number of time steps in each episode <br>\n",
    "> Can be infinite <br>\n",
    "> Otherwise called **finite** Markov reward process\n",
    "\n",
    "*Definition of Return*, $G_t$ (for a Markov Reward Process)\n",
    "> Discounted sum of rewards from time step t to horizon H\n",
    "$$G_t = r_t + \\gamma r_{t+1} + \\gamma^2r_{t+2}+...+\\gamma^{H-1}r_{t+H-1}$$\n",
    "\n",
    "*Definition of State Value Function* $V(s)% (for a Markov Reward Process)\n",
    ">Expected return from starting in state s\n",
    "$$V(s) = \\mathbb{E}[G_t | s_t=s] = \\mathbb{E} [r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2}+ ...+ \\gamma^{H-1} r_{t+H-1} | s_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550bae76-4438-44bf-b7ca-2278c591e48f",
   "metadata": {},
   "source": [
    "**Discount Factor**\n",
    "1. Mathematically convenient (avoid infinite returns and values)\n",
    "2. Humans often act as if there's a discount factor < 1\n",
    "3. $\\gamma=0$ Only care about immediate reward\n",
    "4. $\\gamma=1$ Future reward is as beneficial as immediate reward\n",
    "5. If episode length are always finite ($H < \\infty$), can use $\\gamma =1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a4d3b-a081-4dda-bbb1-690c8380082c",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79047c2f-eb5f-49d7-8f2f-c4cc2bf571fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
