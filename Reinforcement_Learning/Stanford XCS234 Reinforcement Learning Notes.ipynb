{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80761663-dbf4-42b3-b60d-7ccbb597cb1f",
   "metadata": {},
   "source": [
    "This is summary note for [Stanford XCS234 Reinforcement Learning](https://web.stanford.edu/class/cs234/). It contains some of my understanding and inference from learning process, while most of the notes are directly from the notes in course or the book [Reinforcement Learning, Sutton and Barto ](http://incompleteideas.net/book/the-book-2nd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d80aad-fa38-4864-a1b7-a0375864243b",
   "metadata": {},
   "source": [
    "# Module 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a04569-38b1-428a-8f00-efd403219718",
   "metadata": {},
   "source": [
    "## Application of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad0658-1ec8-4355-a81d-2c908344510e",
   "metadata": {},
   "source": [
    "**Reading Material:**\n",
    "- Ch.1 of **[Reinforcement Learning, Sutton and Barto ](http://incompleteideas.net/book/the-book-2nd.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6261d-93e0-4519-a65b-df685eafe2ec",
   "metadata": {},
   "source": [
    "**Reinforcement Learning** learns through experience or data to make good decisions under uncertainty. A learning **agent** must be able to sense the **state** of its **enviornment** to some extent and take **actions** to affect the state with a **goal** or goals related to the state of enviornment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e4826-e320-4158-86b0-dacf9a187e32",
   "metadata": {},
   "source": [
    "**Application**\n",
    "- Chess/Game: Go\n",
    "- Fusion Science: Learning Plasma Control\n",
    "- Covid Testing: Efficient and Targeted\n",
    "- ChatGPT\n",
    "    - Step 1: behavior Cloning, Imitation Learning\n",
    "    - Step 2: Model of Reward, Model-based RL\n",
    "    - Step 3: Reinforcement Learning, RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf7f0a-ce0e-49b7-8eed-d89902ecadcb",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d59caa-0041-477b-ace5-fce118f4d3db",
   "metadata": {},
   "source": [
    "- Optimization: Find the optimal way to make decisions\n",
    "- Delayed Consequences: Decisions now can impact things later; temporal credit is hard.\n",
    "- Exploration: Learning about the world by making decisions; You only get the result of what you try.\n",
    "- Generalization: Policy is mapping from past experience to action\n",
    "    - pre-program policies might be too large to cover all possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bb8d1-1ed7-4a0a-86b9-bc73b453a131",
   "metadata": {},
   "source": [
    "**Imitation Learning (IL)** assumes input demonstrations of good policies. It allows us to reducec RL to Supervised Learning (SL). For example, instead of code a policy for auto-driving, we can use data that has human driving it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f170fe-ff4b-4ecd-804d-4880a290d89d",
   "metadata": {},
   "source": [
    "**When RL is powerful**\n",
    "- No examples of desired behavior e.g. beyond human performance or no existing data\n",
    "- Enormous search or optimization problem with delayed outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e0673-9fd9-4691-9ca6-81caa093768f",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc6f17-7645-4455-b032-0f9475dd5b40",
   "metadata": {},
   "source": [
    "- State\n",
    "- Actions/Decisions\n",
    "- Reward model\n",
    "- Meaning of dynamics model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260269de-bc91-4991-88a7-47a2595edf3a",
   "metadata": {},
   "source": [
    "**Warning**: Reward Hacking - Choosing the easiest solution to maximize the reward but not meaningful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "160304ee-bc33-4d71-986c-717317d3b220",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img height=\"100%\" width=\"50%\" src=\"sources/XCS224_m1_p4_0.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af031fe-bd3c-4841-9421-deeb437ed2f4",
   "metadata": {},
   "source": [
    "**Two Object**\n",
    "- World/Environment\n",
    "- Agent: Our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e983b7-2bda-4667-ac83-e90ba3bbdde3",
   "metadata": {},
   "source": [
    "**Four Sub-element**\n",
    "- **policy**: It defines an agent's behavior at a given time and maps the state of the environment to actions.\n",
    "- **reward signal** (immediate): It defines the goal of a reinforcement learning problem. At each timestep, a reward (scalar value) is sent to an agent from the environment. A large number indicates good **actions**, and a small number indicates bad actions. If the actions are bad, the policy may be changed to select other actions next time. **Agent** aims to maximize its total rewards over the long run. \n",
    "- **value function**: it specifies what is good in the long run. A value of a **state** is the total amount of reward an agent can expect to accumulate over the future, starting from the current stage **(future expectations)**\n",
    "- **model of environment**: It is used to mimic the behavior of environment. The next state and next reward can be inferenced by model. **Model-based methods** are algorithms that use models of the environment where, whereas **Model-free methods** are simpler trial-and-error learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202dc5ed-c6c9-4350-85f6-1b8fb3cc2351",
   "metadata": {},
   "source": [
    "# Module 2: Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a6800-f829-470a-ba20-369d179d0b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
